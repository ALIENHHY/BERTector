{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "# data = pd.read_csv(\"./data/sft/NSL-KDD-100000-sft.csv\")\n",
    "data = pd.read_csv(\"./data/sft/Mixed-sft-400000.csv\")\n",
    "# 划分训练集和验证集\n",
    "train_texts, eval_texts, train_labels, eval_labels = train_test_split(\n",
    "    data[\"flow\"].tolist(), data[\"class\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "# 转换为 Hugging Face Dataset 格式\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"text\": train_texts, \"label\": train_labels}),\n",
    "    \"eval\": Dataset.from_dict({\"text\": eval_texts, \"label\": eval_labels})\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实际设定的 max_length: 65\n"
     ]
    }
   ],
   "source": [
    "# 遍历数据，找到最长文本的长度（基于逗号分词）\n",
    "text_lengths = [len(text.split(\",\")) for text in data[\"flow\"].tolist()]\n",
    "max_length = max(text_lengths)\n",
    "print(f\"实际设定的 max_length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_34496\\419133486.py\", line 1, in <module>\n",
      "    from transformers import BertTokenizer\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\transformers\\__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\transformers\\dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\transformers\\utils\\__init__.py\", line 27, in <module>\n",
      "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\transformers\\utils\\chat_template_utils.py\", line 40, in <module>\n",
      "    from torch import Tensor\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\torch\\__init__.py\", line 1382, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\torch\\functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# 加载 BERT 分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# 逗号分词 + 重新转换为 BERT 需要的 `input_ids`\n",
    "def custom_tokenize_function(examples):\n",
    "    # 逗号分词\n",
    "    tokenized_texts = [text.split(\",\") for text in examples[\"text\"]]\n",
    "    # 将每个短语转换为 BERT input_ids\n",
    "    encodings = tokenizer(\n",
    "        tokenized_texts,  # 逗号分词后的文本\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True  # 关键参数：告诉 tokenizer 文本已经被手动分词\n",
    "    )\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = dataset.map(custom_tokenize_function, batched=True)\n",
    "tokenized_datasets = load_from_disk(\"./tokenized_datasets-100000\")\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "eval_dataset = tokenized_datasets[\"eval\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2, hidden_dropout_prob=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "# **添加 LoRA 适配**\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA 低秩矩阵的秩\n",
    "    lora_alpha=16,  # LoRA scaling factor\n",
    "    lora_dropout=0.1,  # Dropout 防止过拟合\n",
    "    target_modules=[\"query\", \"value\"],  # 仅优化 Query 和 Value 层\n",
    "    task_type=\"SEQ_CLS\",  # 序列分类任务\n",
    ")\n",
    "# 将 LoRA 集成到 BERT 模型\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "lora_model.print_trainable_parameters()  # 查看可训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BertForSequenceClassification(\n",
       "      (bert): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (token_type_embeddings): Embedding(2, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): BertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 继续训练模型\n",
    "lora_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "# 计算评估指标\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    return {\n",
    "        \"eval_loss\": float(np.mean(logits)),  # 确保 `eval_loss` 存在\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer-sft-diy-lora-mixed-400000_1\",\n",
    "    # output_dir=\"./trainer/test_trainer-sft-diy-lora\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # 每个 epoch 保存一次模型\n",
    "    save_steps=None,  # 取消按steps保存\n",
    "    learning_rate=2e-5,  # 学习率  2e-5    5e-5\n",
    "    per_device_train_batch_size=64,  # 适当增加 batch_size，默认 8\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,  # 降低训练轮数，避免过拟合\n",
    "    weight_decay=0.02,  # 加入 L2 正则化\n",
    "    load_best_model_at_end=True,  # 训练结束后加载最佳模型\n",
    "    logging_strategy=\"epoch\",  # 确保每个 epoch 打印 loss\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # 早停机制\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\test\\new\\train-sft-diy-lora.ipynb 单元格 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/test/new/train-sft-diy-lora.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2242\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   2243\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   2244\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   2245\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   2246\u001b[0m     )\n",
      "File \u001b[1;32mc:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\transformers\\trainer.py:2500\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2498\u001b[0m update_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   2499\u001b[0m num_batches \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39mif\u001b[39;00m update_step \u001b[39m!=\u001b[39m (total_updates \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39melse\u001b[39;00m remainder\n\u001b[1;32m-> 2500\u001b[0m batch_samples, num_items_in_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_batch_samples(epoch_iterator, num_batches)\n\u001b[0;32m   2501\u001b[0m \u001b[39mfor\u001b[39;00m i, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2502\u001b[0m     step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\transformers\\trainer.py:5180\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5178\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_batches):\n\u001b[0;32m   5179\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 5180\u001b[0m         batch_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39mnext\u001b[39;49m(epoch_iterator)]\n\u001b[0;32m   5181\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m   5182\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\accelerate\\data_loader.py:564\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 564\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloader_iter)\n\u001b[0;32m    565\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    566\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32mc:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:620\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_next_index\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 620\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sampler_iter)\n",
      "File \u001b[1;32mc:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\torch\\utils\\data\\sampler.py:283\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m batch \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m    282\u001b[0m idx_in_batch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 283\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler:\n\u001b[0;32m    284\u001b[0m     batch[idx_in_batch] \u001b[39m=\u001b[39m idx\n\u001b[0;32m    285\u001b[0m     idx_in_batch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\accelerate\\data_loader.py:99\u001b[0m, in \u001b[0;36mSeedableRandomSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m# print(\"Setting seed at epoch\", self.epoch, seed)\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator\u001b[39m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m---> 99\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m    100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_epoch(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\School\\Anaconda\\envs\\LLMNIDS\\lib\\site-packages\\torch\\utils\\data\\sampler.py:165\u001b[0m, in \u001b[0;36mRandomSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n):\n\u001b[1;32m--> 165\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, torch\u001b[39m.\u001b[39;49mrandperm(n, generator\u001b[39m=\u001b[39;49mgenerator)\u001b[39m.\u001b[39;49mnumpy())\n\u001b[0;32m    166\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, torch\u001b[39m.\u001b[39mrandperm(n, generator\u001b[39m=\u001b[39mgenerator)[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m%\u001b[39m n]\u001b[39m.\u001b[39mnumpy())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
