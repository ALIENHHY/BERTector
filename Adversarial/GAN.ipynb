{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================\n",
    "# 1. 读取数据并做逗号分割\n",
    "#=================================================\n",
    "data_file = \"NSL-KDD-Test.csv\"  # 请修改为你的实际文件路径\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "# 假设文件至少包含 'flow' 列和 'class' 列\n",
    "flows = df[\"flow\"].astype(str).tolist()\n",
    "labels = df[\"class\"].tolist()\n",
    "\n",
    "# flows 每一行示例: \"13,tcp,telnet,SF,118,2425,...\" \n",
    "# 先用逗号切分成列表\n",
    "split_flows = [flow_str.split(\",\") for flow_str in flows]\n",
    "# 现在 split_flows[i] 可能是 [\"13\", \"tcp\", \"telnet\", \"SF\", ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4714\n"
     ]
    }
   ],
   "source": [
    "#=================================================\n",
    "# 2. 建立 token 词表（跳过逗号）\n",
    "#=================================================\n",
    "# 收集所有出现的 token\n",
    "all_tokens = set()\n",
    "for token_list in split_flows:\n",
    "    for tk in token_list:\n",
    "        all_tokens.add(tk.strip())\n",
    "\n",
    "# 可以加入特殊 PAD/UNK 等标记\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "all_tokens = list(all_tokens)\n",
    "all_tokens.sort()  # 为了保证可复现，先排序\n",
    "all_tokens = [PAD_TOKEN, UNK_TOKEN] + all_tokens\n",
    "\n",
    "token2idx = {tk: i for i, tk in enumerate(all_tokens)}\n",
    "idx2token = {i: tk for tk, i in token2idx.items()}\n",
    "\n",
    "vocab_size = len(all_tokens)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================\n",
    "# 3. 编码 & 解码 函数\n",
    "#=================================================\n",
    "# 可以设置一个最大 token 长度(依据数据分布进行调整)\n",
    "max_length = 50\n",
    "\n",
    "def encode_tokens(token_list):\n",
    "    \"\"\"将 token 列表编码为固定长度的索引序列\"\"\"\n",
    "    idx_list = []\n",
    "    for tk in token_list[:max_length]:\n",
    "        if tk in token2idx:\n",
    "            idx_list.append(token2idx[tk])\n",
    "        else:\n",
    "            idx_list.append(token2idx[UNK_TOKEN])\n",
    "    # 不足补PAD\n",
    "    while len(idx_list) < max_length:\n",
    "        idx_list.append(token2idx[PAD_TOKEN])\n",
    "    return idx_list\n",
    "\n",
    "def decode_tokens(idx_list):\n",
    "    \"\"\"将索引序列还原为 token 列表（去掉末尾的 PAD）\"\"\"\n",
    "    tokens = []\n",
    "    for idx in idx_list:\n",
    "        if idx == token2idx[PAD_TOKEN]:\n",
    "            break\n",
    "        tokens.append(idx2token[idx])\n",
    "    return tokens\n",
    "\n",
    "encoded_flows = [encode_tokens(tk_list) for tk_list in split_flows]\n",
    "encoded_flows = np.array(encoded_flows, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#=================================================\n",
    "# 4. 构建 PyTorch Dataset\n",
    "#=================================================\n",
    "class FlowDataset(Dataset):\n",
    "    def __init__(self, flows_idx):\n",
    "        self.flows_idx = flows_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.flows_idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.flows_idx[idx]\n",
    "\n",
    "dataset = FlowDataset(encoded_flows)\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================\n",
    "# 5. 定义生成器 (Generator) 和 判别器 (Discriminator)\n",
    "#=================================================\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    简易序列生成器：将随机噪声映射到 token 序列\n",
    "    用一个 LSTM 将噪声输入映射到 vocab_size 维度\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        z: [batch_size, max_length, latent_dim]\n",
    "        \"\"\"\n",
    "        h, _ = self.lstm(z)    # -> [batch_size, max_length, hidden_dim]\n",
    "        logits = self.fc(h)    # -> [batch_size, max_length, vocab_size]\n",
    "        return logits\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    简易序列判别器：判断序列是真/假\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=token2idx[PAD_TOKEN])\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, max_length] 的 token 索引序列\n",
    "        \"\"\"\n",
    "        emb = self.embedding(x)  # -> [batch_size, max_length, embed_dim]\n",
    "        h, _ = self.lstm(emb)    # -> [batch_size, max_length, hidden_dim]\n",
    "        # 取最后时刻 hidden state 进行二分类\n",
    "        out = h[:, -1, :]\n",
    "        logit = self.fc(out)     # -> [batch_size, 1]\n",
    "        prob = self.sigmoid(logit)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================\n",
    "# 6. 初始化模型与训练参数\n",
    "#=================================================\n",
    "latent_dim = 32\n",
    "g_hidden_dim = 64\n",
    "d_hidden_dim = 64\n",
    "embed_dim = 32\n",
    "num_epochs = 5\n",
    "lr = 0.0002\n",
    "\n",
    "generator = Generator(latent_dim, g_hidden_dim, vocab_size).to(device)\n",
    "discriminator = Discriminator(vocab_size, embed_dim, d_hidden_dim).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] | d_loss: 0.0251, g_loss: 4.1674\n",
      "Epoch [2/5] | d_loss: 0.0071, g_loss: 5.4155\n",
      "Epoch [3/5] | d_loss: 0.0034, g_loss: 6.1989\n",
      "Epoch [4/5] | d_loss: 0.0020, g_loss: 6.8289\n",
      "Epoch [5/5] | d_loss: 0.0011, g_loss: 7.2434\n"
     ]
    }
   ],
   "source": [
    "#=================================================\n",
    "# 7. 训练GAN\n",
    "#=================================================\n",
    "for epoch in range(num_epochs):\n",
    "    for real_batch in dataloader:\n",
    "        real_batch = real_batch.to(device)  # [batch_size, max_length]\n",
    "\n",
    "        #======= 训练判别器 D =======#\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        # 对真实样本打上label=1\n",
    "        real_labels = torch.ones((real_batch.size(0), 1), device=device)\n",
    "        # 对生成样本打上label=0\n",
    "        fake_labels = torch.zeros((real_batch.size(0), 1), device=device)\n",
    "\n",
    "        # (1) 判别真实样本\n",
    "        real_output = discriminator(real_batch)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "\n",
    "        # (2) 生成“假”样本并判别\n",
    "        z = torch.randn(real_batch.size(0), max_length, latent_dim, device=device)\n",
    "        fake_logits = generator(z)  # -> [batch_size, max_length, vocab_size]\n",
    "\n",
    "        # 从 fake_logits 中采样或 argmax\n",
    "        fake_probs = torch.softmax(fake_logits, dim=-1)\n",
    "        fake_indices = torch.argmax(fake_probs, dim=-1)  # -> [batch_size, max_length]\n",
    "\n",
    "        fake_output = discriminator(fake_indices)\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        #======= 训练生成器 G =======#\n",
    "        optimizer_g.zero_grad()\n",
    "        z = torch.randn(real_batch.size(0), max_length, latent_dim, device=device)\n",
    "        fake_logits = generator(z)\n",
    "        fake_probs = torch.softmax(fake_logits, dim=-1)\n",
    "        fake_indices = torch.argmax(fake_probs, dim=-1)\n",
    "\n",
    "        # 生成器希望骗过判别器，因此希望判别器输出接近1\n",
    "        g_output = discriminator(fake_indices)\n",
    "        g_loss = criterion(g_output, real_labels)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成新的对抗流量，并保存到: NSL-KDD_GAN_generated.csv\n"
     ]
    }
   ],
   "source": [
    "#=================================================\n",
    "# 8. 用训练好的生成器生成新flow并写回CSV\n",
    "#=================================================\n",
    "generator.eval()\n",
    "\n",
    "new_flows = []\n",
    "gen_batch_size = 64\n",
    "total_samples = len(df)\n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    while i < total_samples:\n",
    "        cur_batch_size = min(gen_batch_size, total_samples - i)\n",
    "        z = torch.randn(cur_batch_size, max_length, latent_dim, device=device)\n",
    "        fake_logits = generator(z)\n",
    "        fake_probs = torch.softmax(fake_logits, dim=-1)\n",
    "        fake_indices = torch.argmax(fake_probs, dim=-1)\n",
    "\n",
    "        # 解码为 token 列表，再用逗号拼接\n",
    "        for idx_seq in fake_indices:\n",
    "            idx_seq = idx_seq.cpu().numpy().tolist()\n",
    "            tokens = decode_tokens(idx_seq)\n",
    "            flow_str = \",\".join(tokens)  # 用逗号拼接回来\n",
    "            new_flows.append(flow_str)\n",
    "\n",
    "        i += cur_batch_size\n",
    "\n",
    "# 与原数据长度一致\n",
    "assert len(new_flows) == len(df)\n",
    "\n",
    "# 组合成新的数据\n",
    "new_df = pd.DataFrame({\n",
    "    \"flow\": new_flows,\n",
    "    \"class\": labels\n",
    "})\n",
    "\n",
    "output_file = \"NSL-KDD-GAN-Adversarial.csv\"\n",
    "new_df.to_csv(output_file, index=False)\n",
    "print(f\"已生成新的对抗流量，并保存到: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
